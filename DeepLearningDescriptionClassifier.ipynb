{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kingalandydy/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np\n",
    "import math as math\n",
    "from nltk.corpus import stopwords \n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "import operator\n",
    "import collections\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import spacy\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from time import time \n",
    "import multiprocessing\n",
    "from gensim.models import Word2Vec\n",
    "import bokeh.plotting as bp\n",
    "from bokeh.models import HoverTool, BoxSelectTool\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import scale\n",
    "import keras \n",
    "from keras.models import Sequential, Model \n",
    "from keras import layers\n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, Input, Embedding\n",
    "from keras.layers.merge import Concatenate\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>energy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>The Company is a France-based electricity prod...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>The Company was created on November 27, 1962 I...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.0</th>\n",
       "      <td>The Company is a distributor of natural gas. N...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7.0</th>\n",
       "      <td>The Company specializes in electricity and gas...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8.0</th>\n",
       "      <td>RWE is the electricity and gas companies. Thro...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           description  energy\n",
       "4.0  The Company is a France-based electricity prod...       1\n",
       "5.0  The Company was created on November 27, 1962 I...       1\n",
       "6.0  The Company is a distributor of natural gas. N...       1\n",
       "7.0  The Company specializes in electricity and gas...       1\n",
       "8.0  RWE is the electricity and gas companies. Thro...       1"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xls = pd.ExcelFile('./Orbis - Energy.xlsx')\n",
    "df = pd.read_excel(xls, 'Results')\n",
    "#remove ones without description\n",
    "df = df[~ pd.isnull(df['Description and history'])]\n",
    "#remove the history\n",
    "df['description'] = df['Description and history'].str.split('history').str[0]\n",
    "df[\"energy\"] = 1\n",
    "df = df[['description','energy']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>energy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>AT&amp;T Inc., incorporated October 5, 1983, is a ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.0</th>\n",
       "      <td>The company is engaged in various sectors, inc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8.0</th>\n",
       "      <td>Alphabet Inc., incorporated on July 23, 2015, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.0</th>\n",
       "      <td>Verizon Communications Inc., incorporated on O...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14.0</th>\n",
       "      <td>The Company is a Japan-based telecommunication...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            description  energy\n",
       "4.0   AT&T Inc., incorporated October 5, 1983, is a ...       0\n",
       "6.0   The company is engaged in various sectors, inc...       0\n",
       "8.0   Alphabet Inc., incorporated on July 23, 2015, ...       0\n",
       "10.0  Verizon Communications Inc., incorporated on O...       0\n",
       "14.0  The Company is a Japan-based telecommunication...       0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xls = pd.ExcelFile('./Orbis - Non-Energy.xlsx')\n",
    "df2 = pd.read_excel(xls, 'Results')\n",
    "#remove ones without description\n",
    "df2 = df2[~ pd.isnull(df2['Description and history'])]\n",
    "#remove the history\n",
    "df2['description'] = df2['Description and history'].str.split('history').str[0]\n",
    "df2[\"energy\"] = 0\n",
    "df2 = df2[['description','energy']]\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>energy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>The Company is a France-based electricity prod...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>The Company was created on November 27, 1962 I...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.0</th>\n",
       "      <td>The Company is a distributor of natural gas. N...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7.0</th>\n",
       "      <td>The Company specializes in electricity and gas...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8.0</th>\n",
       "      <td>RWE is the electricity and gas companies. Thro...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           description  energy\n",
       "4.0  The Company is a France-based electricity prod...       1\n",
       "5.0  The Company was created on November 27, 1962 I...       1\n",
       "6.0  The Company is a distributor of natural gas. N...       1\n",
       "7.0  The Company specializes in electricity and gas...       1\n",
       "8.0  RWE is the electricity and gas companies. Thro...       1"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfx = pd.concat([df,df2])\n",
    "df = dfx\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "descList = []\n",
    "df['description'] = df['description'].astype(str)\n",
    "df['description'] = df['description'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "df['description'] = df['description'].apply(lambda x: ' '.join([stemmer.stem(y) for y in x.split(' ')]))\n",
    "for description in df['description'].iteritems():\n",
    "    descList.append(nltk.word_tokenize(description[1].lower()))\n",
    "df['tokenized'] = descList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>energy</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>the compani france-bas electr producer, market...</td>\n",
       "      <td>1</td>\n",
       "      <td>[the, compani, france-bas, electr, producer, ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>the compani creat novemb 27, 1962 it oper seve...</td>\n",
       "      <td>1</td>\n",
       "      <td>[the, compani, creat, novemb, 27, ,, 1962, it,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.0</th>\n",
       "      <td>the compani distributor natur gas. net sale br...</td>\n",
       "      <td>1</td>\n",
       "      <td>[the, compani, distributor, natur, gas, ., net...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7.0</th>\n",
       "      <td>the compani special electr gas product relat s...</td>\n",
       "      <td>1</td>\n",
       "      <td>[the, compani, special, electr, gas, product, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8.0</th>\n",
       "      <td>rwe electr gas companies. through expertis oil...</td>\n",
       "      <td>1</td>\n",
       "      <td>[rwe, electr, gas, companies, ., through, expe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           description  energy  \\\n",
       "4.0  the compani france-bas electr producer, market...       1   \n",
       "5.0  the compani creat novemb 27, 1962 it oper seve...       1   \n",
       "6.0  the compani distributor natur gas. net sale br...       1   \n",
       "7.0  the compani special electr gas product relat s...       1   \n",
       "8.0  rwe electr gas companies. through expertis oil...       1   \n",
       "\n",
       "                                             tokenized  \n",
       "4.0  [the, compani, france-bas, electr, producer, ,...  \n",
       "5.0  [the, compani, creat, novemb, 27, ,, 1962, it,...  \n",
       "6.0  [the, compani, distributor, natur, gas, ., net...  \n",
       "7.0  [the, compani, special, electr, gas, product, ...  \n",
       "8.0  [rwe, electr, gas, companies, ., through, expe...  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "w2v_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I tried training my own model but it was not enough words\n",
    "#WORD2VEC()\n",
    "#cores = multiprocessing.cpu_count() # Count the number of cores in a computer, important for a parameter of the model\n",
    "#w2v_model = Word2Vec(min_count=20,\n",
    "#                     window=2,\n",
    "#                     size=300,\n",
    "#                     sample=6e-5, \n",
    "#                     alpha=0.03, \n",
    "#                     min_alpha=0.0007, \n",
    "#                     negative=20,\n",
    "#                     workers=cores-1)\n",
    "\n",
    "#BUILD_VOCAB()\n",
    "#t = time()\n",
    "#w2v_model.build_vocab(df_clean[\"tokens\"], progress_per=1000)\n",
    "#print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))\n",
    "\n",
    "#TRAIN()\n",
    "#w2v_model.train(df_clean[\"tokens\"], total_examples=w2v_model.corpus_count, epochs=10000, report_delay=1)\n",
    "#print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting data\n",
    "#First defining the X (input), and the y (output)\n",
    "y = df['energy'].values\n",
    "X = np.array(df[\"tokenized\"])\n",
    "\n",
    "#And here is the train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.05, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size : 10922\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer=lambda x: x, min_df=10)\n",
    "matrix = vectorizer.fit_transform([x for x in X_train])\n",
    "tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n",
    "print ('vocab size :', len(tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildWordVector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += w2v_model[word].reshape((1, size)) * tfidf[word]\n",
    "            count += 1.\n",
    "        except KeyError: # handling the case where the token is not\n",
    "                         # in the corpus. useful for testing.\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape for training set :  (17743, 300) \n",
      "shape for test set :  (934, 300)\n"
     ]
    }
   ],
   "source": [
    "train_vecs_w2v = np.concatenate([buildWordVector(z, 300) for z in map(lambda x: x, X_train)])\n",
    "train_vecs_w2v = scale(train_vecs_w2v)\n",
    "\n",
    "test_vecs_w2v = np.concatenate([buildWordVector(z, 300) for z in map(lambda x: x, X_test)])\n",
    "test_vecs_w2v = scale(test_vecs_w2v)\n",
    "\n",
    "print ('shape for training set : ',train_vecs_w2v.shape,\n",
    "      '\\nshape for test set : ', test_vecs_w2v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_13 (Dense)             (None, 128)               38528     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 38,657\n",
      "Trainable params: 38,657\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(128, activation='relu', input_dim=300))\n",
    "model.add(Dropout(0.8))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adadelta',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17743 samples, validate on 934 samples\n",
      "Epoch 1/30\n",
      "17743/17743 [==============================] - 4s 245us/step - loss: 0.2918 - accuracy: 0.9013 - val_loss: 0.1112 - val_accuracy: 0.9679\n",
      "Epoch 2/30\n",
      "17743/17743 [==============================] - 3s 171us/step - loss: 0.1798 - accuracy: 0.9474 - val_loss: 0.1052 - val_accuracy: 0.9700\n",
      "Epoch 3/30\n",
      "17743/17743 [==============================] - 3s 174us/step - loss: 0.1524 - accuracy: 0.9531 - val_loss: 0.1026 - val_accuracy: 0.9711\n",
      "Epoch 4/30\n",
      "17743/17743 [==============================] - 3s 179us/step - loss: 0.1481 - accuracy: 0.9553 - val_loss: 0.1035 - val_accuracy: 0.9700\n",
      "Epoch 5/30\n",
      "17743/17743 [==============================] - 3s 178us/step - loss: 0.1412 - accuracy: 0.9565 - val_loss: 0.0987 - val_accuracy: 0.9690\n",
      "Epoch 6/30\n",
      "17743/17743 [==============================] - 3s 186us/step - loss: 0.1367 - accuracy: 0.9577 - val_loss: 0.0991 - val_accuracy: 0.9679\n",
      "Epoch 7/30\n",
      "17743/17743 [==============================] - 3s 194us/step - loss: 0.1300 - accuracy: 0.9609 - val_loss: 0.0993 - val_accuracy: 0.9679\n",
      "Epoch 8/30\n",
      "17743/17743 [==============================] - 3s 191us/step - loss: 0.1295 - accuracy: 0.9613 - val_loss: 0.0992 - val_accuracy: 0.9679\n",
      "Epoch 9/30\n",
      "17743/17743 [==============================] - 3s 192us/step - loss: 0.1223 - accuracy: 0.9636 - val_loss: 0.0979 - val_accuracy: 0.9679\n",
      "Epoch 10/30\n",
      "17743/17743 [==============================] - 3s 192us/step - loss: 0.1184 - accuracy: 0.9639 - val_loss: 0.0977 - val_accuracy: 0.9668\n",
      "Epoch 11/30\n",
      "17743/17743 [==============================] - 3s 194us/step - loss: 0.1186 - accuracy: 0.9620 - val_loss: 0.0968 - val_accuracy: 0.9679\n",
      "Epoch 12/30\n",
      "17743/17743 [==============================] - 3s 192us/step - loss: 0.1111 - accuracy: 0.9655 - val_loss: 0.0952 - val_accuracy: 0.9711\n",
      "Epoch 13/30\n",
      "17743/17743 [==============================] - 3s 193us/step - loss: 0.1144 - accuracy: 0.9657 - val_loss: 0.0935 - val_accuracy: 0.9700\n",
      "Epoch 14/30\n",
      "17743/17743 [==============================] - 4s 199us/step - loss: 0.1164 - accuracy: 0.9655 - val_loss: 0.0958 - val_accuracy: 0.9711\n",
      "Epoch 15/30\n",
      "17743/17743 [==============================] - 3s 194us/step - loss: 0.1124 - accuracy: 0.9661 - val_loss: 0.0955 - val_accuracy: 0.9700\n",
      "Epoch 16/30\n",
      "17743/17743 [==============================] - 3s 192us/step - loss: 0.1062 - accuracy: 0.9676 - val_loss: 0.0987 - val_accuracy: 0.9679\n",
      "Epoch 17/30\n",
      "17743/17743 [==============================] - 3s 192us/step - loss: 0.1084 - accuracy: 0.9670 - val_loss: 0.1003 - val_accuracy: 0.9657\n",
      "Epoch 18/30\n",
      "17743/17743 [==============================] - 3s 191us/step - loss: 0.1056 - accuracy: 0.9688 - val_loss: 0.0990 - val_accuracy: 0.9690\n",
      "Epoch 19/30\n",
      "17743/17743 [==============================] - 3s 189us/step - loss: 0.1077 - accuracy: 0.9667 - val_loss: 0.0976 - val_accuracy: 0.9690\n",
      "Epoch 20/30\n",
      "17743/17743 [==============================] - 4s 198us/step - loss: 0.1046 - accuracy: 0.9689 - val_loss: 0.0986 - val_accuracy: 0.9679\n",
      "Epoch 21/30\n",
      "17743/17743 [==============================] - 3s 190us/step - loss: 0.1043 - accuracy: 0.9671 - val_loss: 0.1001 - val_accuracy: 0.9700\n",
      "Epoch 22/30\n",
      "17743/17743 [==============================] - 3s 187us/step - loss: 0.1008 - accuracy: 0.9687 - val_loss: 0.0985 - val_accuracy: 0.9711\n",
      "Epoch 23/30\n",
      "17743/17743 [==============================] - 3s 188us/step - loss: 0.1038 - accuracy: 0.9684 - val_loss: 0.0981 - val_accuracy: 0.9700\n",
      "Epoch 24/30\n",
      "17743/17743 [==============================] - 3s 195us/step - loss: 0.1005 - accuracy: 0.9694 - val_loss: 0.1004 - val_accuracy: 0.9711\n",
      "Epoch 25/30\n",
      "17743/17743 [==============================] - 3s 186us/step - loss: 0.1026 - accuracy: 0.9696 - val_loss: 0.0981 - val_accuracy: 0.9711\n",
      "Epoch 26/30\n",
      "17743/17743 [==============================] - 4s 198us/step - loss: 0.0958 - accuracy: 0.9705 - val_loss: 0.1005 - val_accuracy: 0.9700\n",
      "Epoch 27/30\n",
      "17743/17743 [==============================] - 4s 209us/step - loss: 0.0984 - accuracy: 0.9705 - val_loss: 0.0953 - val_accuracy: 0.9690\n",
      "Epoch 28/30\n",
      "17743/17743 [==============================] - 4s 198us/step - loss: 0.0982 - accuracy: 0.9686 - val_loss: 0.0981 - val_accuracy: 0.9700\n",
      "Epoch 29/30\n",
      "17743/17743 [==============================] - 3s 193us/step - loss: 0.0965 - accuracy: 0.9699 - val_loss: 0.1014 - val_accuracy: 0.9700\n",
      "Epoch 30/30\n",
      "17743/17743 [==============================] - 3s 188us/step - loss: 0.0991 - accuracy: 0.9706 - val_loss: 0.1007 - val_accuracy: 0.9711\n",
      "Training Accuracy: 0.9788\n",
      "Testing Accuracy:  0.9711\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_vecs_w2v, y_train, epochs=30, batch_size=40,\n",
    "                   validation_data=(test_vecs_w2v,y_test))\n",
    "loss, accuracy = model.evaluate(train_vecs_w2v, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(test_vecs_w2v, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3741498 words total, with a vocabulary size of 130977\n",
      "Max sentence length is 4338\n"
     ]
    }
   ],
   "source": [
    "all_words = [word for tokens in X for word in tokens]\n",
    "all_sentence_lengths = [len(tokens) for tokens in X]\n",
    "ALL_VOCAB = sorted(list(set(all_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_words), len(ALL_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(all_sentence_lengths))\n",
    "\n",
    "\n",
    "####################### CHANGE THE PARAMETERS HERE #####################################\n",
    "EMBEDDING_DIM = 300 # how big is each word vector\n",
    "MAX_VOCAB_SIZE = 18399# how many unique words to use (i.e num rows in embedding vector)\n",
    "MAX_SEQUENCE_LENGTH = 200 # max number of words in a comment to use (was 53)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 98348 unique tokens.\n",
      "(98349, 300)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(df[\"description\"].tolist())\n",
    "training_sequences = tokenizer.texts_to_sequences(X_train.tolist())\n",
    "\n",
    "train_word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(train_word_index))\n",
    "\n",
    "train_embedding_weights = np.zeros((len(train_word_index)+1, EMBEDDING_DIM))\n",
    "for word,index in train_word_index.items():\n",
    "    train_embedding_weights[index,:] = w2v_model[word] if word in w2v_model else np.random.rand(EMBEDDING_DIM)\n",
    "print(train_embedding_weights.shape)\n",
    "\n",
    "\n",
    "######################## TRAIN AND TEST SET #################################\n",
    "train_cnn_data = pad_sequences(training_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_sequences = tokenizer.texts_to_sequences(X_test.tolist())\n",
    "test_cnn_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import concatenate\n",
    "def ConvNet(embeddings, max_sequence_length, num_words, embedding_dim, trainable=False, extra_conv=True):\n",
    "    \n",
    "    embedding_layer = Embedding(num_words,\n",
    "                            embedding_dim,\n",
    "                            weights=[embeddings],\n",
    "                            input_length=max_sequence_length,\n",
    "                            trainable=trainable)\n",
    "\n",
    "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    # Yoon Kim model (https://arxiv.org/abs/1408.5882)\n",
    "    convs = []\n",
    "    filter_sizes = [3,4,5]\n",
    "\n",
    "    for filter_size in filter_sizes:\n",
    "        l_conv = Conv1D(filters=128, kernel_size=filter_size, activation='relu')(embedded_sequences)\n",
    "        l_pool = MaxPooling1D(pool_size=3)(l_conv)\n",
    "        convs.append(l_pool)\n",
    "\n",
    "    l_merge = concatenate([convs[0],convs[1],convs[2]],axis=1)\n",
    "\n",
    "    # add a 1D convnet with global maxpooling, instead of Yoon Kim model\n",
    "    conv = Conv1D(filters=128, kernel_size=3, activation='relu')(embedded_sequences)\n",
    "    pool = MaxPooling1D(pool_size=3)(conv)\n",
    "\n",
    "    if extra_conv==True:\n",
    "        x = Dropout(0.6)(l_merge)  \n",
    "    else:\n",
    "        # Original Yoon Kim model\n",
    "        x = Dropout(0.5)(pool)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    # Finally, we feed the output into a Sigmoid layer.\n",
    "    # The reason why sigmoid is used is because we are trying to achieve a binary classification(1,0) \n",
    "    # for each of the 6 labels, and the sigmoid function will squash the output between the bounds of 0 and 1.\n",
    "    preds = Dense(1,activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adadelta',\n",
    "                  metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, 300, 300)     29504700    input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 298, 128)     115328      embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 297, 128)     153728      embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 296, 128)     192128      embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling1D) (None, 99, 128)      0           conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling1D) (None, 99, 128)      0           conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling1D) (None, 98, 128)      0           conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 296, 128)     0           max_pooling1d_25[0][0]           \n",
      "                                                                 max_pooling1d_26[0][0]           \n",
      "                                                                 max_pooling1d_27[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 296, 128)     0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 37888)        0           dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 128)          4849792     flatten_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 1)            129         dense_19[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 34,815,805\n",
      "Trainable params: 5,311,105\n",
      "Non-trainable params: 29,504,700\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = ConvNet(train_embedding_weights, MAX_SEQUENCE_LENGTH, len(train_word_index)+1, EMBEDDING_DIM, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17743 samples, validate on 934 samples\n",
      "Epoch 1/10\n",
      "17743/17743 [==============================] - 206s 12ms/step - loss: 0.2756 - acc: 0.8960 - val_loss: 0.1215 - val_acc: 0.9593\n",
      "Epoch 2/10\n",
      "17743/17743 [==============================] - 193s 11ms/step - loss: 0.1209 - acc: 0.9587 - val_loss: 0.1337 - val_acc: 0.9657\n",
      "Epoch 3/10\n",
      "17743/17743 [==============================] - 195s 11ms/step - loss: 0.1004 - acc: 0.9660 - val_loss: 0.1045 - val_acc: 0.9679\n",
      "Epoch 4/10\n",
      "17743/17743 [==============================] - 193s 11ms/step - loss: 0.0837 - acc: 0.9711 - val_loss: 0.0963 - val_acc: 0.9754\n",
      "Epoch 5/10\n",
      "17743/17743 [==============================] - 195s 11ms/step - loss: 0.0720 - acc: 0.9758 - val_loss: 0.1070 - val_acc: 0.9722\n",
      "Epoch 6/10\n",
      "17743/17743 [==============================] - 196s 11ms/step - loss: 0.0655 - acc: 0.9775 - val_loss: 0.1094 - val_acc: 0.9700\n",
      "Epoch 7/10\n",
      "17743/17743 [==============================] - 192s 11ms/step - loss: 0.0577 - acc: 0.9804 - val_loss: 0.1574 - val_acc: 0.9647\n",
      "Epoch 8/10\n",
      "17743/17743 [==============================] - 193s 11ms/step - loss: 0.0512 - acc: 0.9827 - val_loss: 0.1167 - val_acc: 0.9743\n",
      "Epoch 9/10\n",
      "17743/17743 [==============================] - 194s 11ms/step - loss: 0.0445 - acc: 0.9847 - val_loss: 0.1484 - val_acc: 0.9711\n",
      "Epoch 10/10\n",
      "17743/17743 [==============================] - 194s 11ms/step - loss: 0.0421 - acc: 0.9851 - val_loss: 0.1820 - val_acc: 0.9679\n",
      "Training Accuracy: 0.9878\n",
      "Testing Accuracy:  0.9679\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_cnn_data, y_train, epochs=10, batch_size=50,\n",
    "                   validation_data=(test_cnn_data,y_test))\n",
    "loss, accuracy = model.evaluate(train_cnn_data, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(test_cnn_data, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
